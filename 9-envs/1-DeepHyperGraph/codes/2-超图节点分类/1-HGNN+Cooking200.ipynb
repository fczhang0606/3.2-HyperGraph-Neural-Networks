{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377f803d-90ac-425f-9211-b77f6e40850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from dhg.data import Cooking200\n",
    "from dhg import Hypergraph\n",
    "from dhg.models import HGNN\n",
    "from dhg.metrics import HypergraphVertexClassificationEvaluator as Evaluator\n",
    "from dhg.random import set_seed\n",
    "\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe3ec37-9dd6-47ad-9890-6e4c8dd1d32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(A, X, lbls, net, train_idx, optimizer, epoch):\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    st = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Time: {time.time()-st:.5f}s, Loss: {loss.item():.5f}\")\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6dee5c6-3368-47fa-b3d7-57c682ebb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer(A, X, lbls, net, idx, test=False):\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8312017a-5bfa-4500-921c-caaece49c972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.31827s, Loss: 2.99596\n",
      "update best: 0.05500\n",
      "Epoch: 1, Time: 0.00766s, Loss: 2.69715\n",
      "Epoch: 2, Time: 0.00779s, Loss: 2.29850\n",
      "Epoch: 3, Time: 0.00755s, Loss: 2.07462\n",
      "Epoch: 4, Time: 0.00776s, Loss: 1.93425\n",
      "Epoch: 5, Time: 0.00778s, Loss: 1.78795\n",
      "Epoch: 6, Time: 0.00756s, Loss: 1.67265\n",
      "Epoch: 7, Time: 0.00776s, Loss: 1.54354\n",
      "Epoch: 8, Time: 0.00751s, Loss: 1.42376\n",
      "update best: 0.07000\n",
      "Epoch: 9, Time: 0.00759s, Loss: 1.31088\n",
      "update best: 0.07500\n",
      "Epoch: 10, Time: 0.00754s, Loss: 1.19915\n",
      "update best: 0.09500\n",
      "Epoch: 11, Time: 0.00736s, Loss: 1.08702\n",
      "update best: 0.10500\n",
      "Epoch: 12, Time: 0.00749s, Loss: 0.99158\n",
      "Epoch: 13, Time: 0.00753s, Loss: 0.89612\n",
      "Epoch: 14, Time: 0.00738s, Loss: 0.81883\n",
      "update best: 0.11000\n",
      "Epoch: 15, Time: 0.00757s, Loss: 0.73588\n",
      "update best: 0.14500\n",
      "Epoch: 16, Time: 0.00734s, Loss: 0.65488\n",
      "Epoch: 17, Time: 0.00740s, Loss: 0.58840\n",
      "Epoch: 18, Time: 0.00757s, Loss: 0.51915\n",
      "Epoch: 19, Time: 0.00742s, Loss: 0.47159\n",
      "Epoch: 20, Time: 0.00759s, Loss: 0.40998\n",
      "Epoch: 21, Time: 0.00779s, Loss: 0.37416\n",
      "Epoch: 22, Time: 0.00802s, Loss: 0.33101\n",
      "Epoch: 23, Time: 0.00774s, Loss: 0.29445\n",
      "Epoch: 24, Time: 0.00733s, Loss: 0.26244\n",
      "Epoch: 25, Time: 0.00739s, Loss: 0.22768\n",
      "Epoch: 26, Time: 0.00758s, Loss: 0.21295\n",
      "Epoch: 27, Time: 0.00739s, Loss: 0.18574\n",
      "update best: 0.15000\n",
      "Epoch: 28, Time: 0.00755s, Loss: 0.16602\n",
      "update best: 0.17000\n",
      "Epoch: 29, Time: 0.00739s, Loss: 0.14692\n",
      "update best: 0.18000\n",
      "Epoch: 30, Time: 0.00738s, Loss: 0.12551\n",
      "update best: 0.19000\n",
      "Epoch: 31, Time: 0.00754s, Loss: 0.11834\n",
      "Epoch: 32, Time: 0.00735s, Loss: 0.10488\n",
      "update best: 0.21000\n",
      "Epoch: 33, Time: 0.00754s, Loss: 0.09729\n",
      "update best: 0.21500\n",
      "Epoch: 34, Time: 0.00759s, Loss: 0.08647\n",
      "update best: 0.22000\n",
      "Epoch: 35, Time: 0.00742s, Loss: 0.07999\n",
      "update best: 0.22500\n",
      "Epoch: 36, Time: 0.00754s, Loss: 0.07032\n",
      "update best: 0.23000\n",
      "Epoch: 37, Time: 0.00739s, Loss: 0.06701\n",
      "Epoch: 38, Time: 0.00737s, Loss: 0.06443\n",
      "update best: 0.24500\n",
      "Epoch: 39, Time: 0.00749s, Loss: 0.05846\n",
      "update best: 0.25500\n",
      "Epoch: 40, Time: 0.00737s, Loss: 0.05283\n",
      "Epoch: 41, Time: 0.00785s, Loss: 0.05099\n",
      "Epoch: 42, Time: 0.00737s, Loss: 0.04904\n",
      "Epoch: 43, Time: 0.00768s, Loss: 0.04553\n",
      "Epoch: 44, Time: 0.00779s, Loss: 0.04610\n",
      "Epoch: 45, Time: 0.00732s, Loss: 0.04389\n",
      "update best: 0.26000\n",
      "Epoch: 46, Time: 0.00743s, Loss: 0.04196\n",
      "Epoch: 47, Time: 0.00747s, Loss: 0.04021\n",
      "Epoch: 48, Time: 0.00740s, Loss: 0.04083\n",
      "Epoch: 49, Time: 0.00751s, Loss: 0.03944\n",
      "Epoch: 50, Time: 0.00741s, Loss: 0.03725\n",
      "Epoch: 51, Time: 0.00787s, Loss: 0.03708\n",
      "update best: 0.27500\n",
      "Epoch: 52, Time: 0.00780s, Loss: 0.03635\n",
      "Epoch: 53, Time: 0.00779s, Loss: 0.03535\n",
      "Epoch: 54, Time: 0.00742s, Loss: 0.03441\n",
      "update best: 0.28000\n",
      "Epoch: 55, Time: 0.00754s, Loss: 0.03667\n",
      "update best: 0.29000\n",
      "Epoch: 56, Time: 0.00739s, Loss: 0.03343\n",
      "update best: 0.30500\n",
      "Epoch: 57, Time: 0.00751s, Loss: 0.03266\n",
      "update best: 0.32500\n",
      "Epoch: 58, Time: 0.00735s, Loss: 0.03229\n",
      "Epoch: 59, Time: 0.00737s, Loss: 0.03343\n",
      "Epoch: 60, Time: 0.00757s, Loss: 0.03220\n",
      "update best: 0.34000\n",
      "Epoch: 61, Time: 0.00769s, Loss: 0.03208\n",
      "Epoch: 62, Time: 0.00808s, Loss: 0.03018\n",
      "update best: 0.34500\n",
      "Epoch: 63, Time: 0.00756s, Loss: 0.03000\n",
      "Epoch: 64, Time: 0.00736s, Loss: 0.03178\n",
      "Epoch: 65, Time: 0.00753s, Loss: 0.02929\n",
      "Epoch: 66, Time: 0.00739s, Loss: 0.02959\n",
      "Epoch: 67, Time: 0.00743s, Loss: 0.03099\n",
      "update best: 0.35500\n",
      "Epoch: 68, Time: 0.00757s, Loss: 0.02871\n",
      "update best: 0.38000\n",
      "Epoch: 69, Time: 0.00741s, Loss: 0.02912\n",
      "update best: 0.40000\n",
      "Epoch: 70, Time: 0.00761s, Loss: 0.02665\n",
      "Epoch: 71, Time: 0.00736s, Loss: 0.02759\n",
      "update best: 0.41000\n",
      "Epoch: 72, Time: 0.00738s, Loss: 0.02696\n",
      "update best: 0.44000\n",
      "Epoch: 73, Time: 0.00759s, Loss: 0.02608\n",
      "Epoch: 74, Time: 0.00741s, Loss: 0.02598\n",
      "Epoch: 75, Time: 0.00743s, Loss: 0.02474\n",
      "Epoch: 76, Time: 0.00762s, Loss: 0.02892\n",
      "Epoch: 77, Time: 0.00736s, Loss: 0.02803\n",
      "Epoch: 78, Time: 0.00759s, Loss: 0.02836\n",
      "Epoch: 79, Time: 0.00733s, Loss: 0.02334\n",
      "Epoch: 80, Time: 0.00744s, Loss: 0.03180\n",
      "Epoch: 81, Time: 0.00778s, Loss: 0.02343\n",
      "update best: 0.44500\n",
      "Epoch: 82, Time: 0.00743s, Loss: 0.02584\n",
      "update best: 0.46500\n",
      "Epoch: 83, Time: 0.00768s, Loss: 0.02521\n",
      "Epoch: 84, Time: 0.00757s, Loss: 0.02268\n",
      "Epoch: 85, Time: 0.00740s, Loss: 0.02344\n",
      "Epoch: 86, Time: 0.00757s, Loss: 0.02440\n",
      "Epoch: 87, Time: 0.00737s, Loss: 0.02382\n",
      "Epoch: 88, Time: 0.00740s, Loss: 0.02177\n",
      "Epoch: 89, Time: 0.00754s, Loss: 0.02147\n",
      "Epoch: 90, Time: 0.00732s, Loss: 0.02179\n",
      "Epoch: 91, Time: 0.00753s, Loss: 0.02130\n",
      "update best: 0.47500\n",
      "Epoch: 92, Time: 0.00732s, Loss: 0.02056\n",
      "Epoch: 93, Time: 0.00733s, Loss: 0.02004\n",
      "update best: 0.48000\n",
      "Epoch: 94, Time: 0.00757s, Loss: 0.01855\n",
      "update best: 0.48500\n",
      "Epoch: 95, Time: 0.00738s, Loss: 0.01808\n",
      "Epoch: 96, Time: 0.00756s, Loss: 0.01845\n",
      "Epoch: 97, Time: 0.00739s, Loss: 0.01841\n",
      "Epoch: 98, Time: 0.00734s, Loss: 0.01731\n",
      "Epoch: 99, Time: 0.00754s, Loss: 0.01765\n",
      "Epoch: 100, Time: 0.00734s, Loss: 0.01752\n",
      "Epoch: 101, Time: 0.00812s, Loss: 0.01765\n",
      "Epoch: 102, Time: 0.00786s, Loss: 0.01714\n",
      "Epoch: 103, Time: 0.00752s, Loss: 0.01807\n",
      "Epoch: 104, Time: 0.00776s, Loss: 0.01670\n",
      "Epoch: 105, Time: 0.00736s, Loss: 0.01688\n",
      "Epoch: 106, Time: 0.00734s, Loss: 0.01722\n",
      "Epoch: 107, Time: 0.00756s, Loss: 0.01680\n",
      "Epoch: 108, Time: 0.00735s, Loss: 0.01673\n",
      "Epoch: 109, Time: 0.00752s, Loss: 0.01714\n",
      "Epoch: 110, Time: 0.00740s, Loss: 0.01663\n",
      "Epoch: 111, Time: 0.00737s, Loss: 0.01669\n",
      "Epoch: 112, Time: 0.00752s, Loss: 0.01782\n",
      "Epoch: 113, Time: 0.00737s, Loss: 0.01722\n",
      "Epoch: 114, Time: 0.00757s, Loss: 0.01766\n",
      "Epoch: 115, Time: 0.00740s, Loss: 0.01675\n",
      "Epoch: 116, Time: 0.00741s, Loss: 0.01765\n",
      "Epoch: 117, Time: 0.00755s, Loss: 0.01565\n",
      "Epoch: 118, Time: 0.00732s, Loss: 0.01617\n",
      "Epoch: 119, Time: 0.00756s, Loss: 0.01705\n",
      "Epoch: 120, Time: 0.00752s, Loss: 0.01558\n",
      "Epoch: 121, Time: 0.00769s, Loss: 0.01568\n",
      "Epoch: 122, Time: 0.00810s, Loss: 0.01568\n",
      "Epoch: 123, Time: 0.00729s, Loss: 0.01562\n",
      "update best: 0.49500\n",
      "Epoch: 124, Time: 0.00738s, Loss: 0.01585\n",
      "Epoch: 125, Time: 0.00756s, Loss: 0.01569\n",
      "Epoch: 126, Time: 0.00736s, Loss: 0.01570\n",
      "Epoch: 127, Time: 0.00757s, Loss: 0.01585\n",
      "Epoch: 128, Time: 0.00738s, Loss: 0.01502\n",
      "Epoch: 129, Time: 0.00736s, Loss: 0.01559\n",
      "Epoch: 130, Time: 0.00763s, Loss: 0.01469\n",
      "Epoch: 131, Time: 0.00743s, Loss: 0.01351\n",
      "Epoch: 132, Time: 0.00764s, Loss: 0.01497\n",
      "Epoch: 133, Time: 0.00788s, Loss: 0.01463\n",
      "Epoch: 134, Time: 0.00781s, Loss: 0.01533\n",
      "Epoch: 135, Time: 0.00781s, Loss: 0.01476\n",
      "Epoch: 136, Time: 0.00739s, Loss: 0.01465\n",
      "Epoch: 137, Time: 0.00744s, Loss: 0.01451\n",
      "update best: 0.50500\n",
      "Epoch: 138, Time: 0.00760s, Loss: 0.01547\n",
      "Epoch: 139, Time: 0.00737s, Loss: 0.01484\n",
      "update best: 0.51500\n",
      "Epoch: 140, Time: 0.00761s, Loss: 0.01333\n",
      "update best: 0.52500\n",
      "Epoch: 141, Time: 0.00778s, Loss: 0.01487\n",
      "Epoch: 142, Time: 0.00773s, Loss: 0.01362\n",
      "Epoch: 143, Time: 0.00774s, Loss: 0.01473\n",
      "Epoch: 144, Time: 0.00737s, Loss: 0.01397\n",
      "Epoch: 145, Time: 0.00743s, Loss: 0.01385\n",
      "Epoch: 146, Time: 0.00753s, Loss: 0.01278\n",
      "Epoch: 147, Time: 0.00734s, Loss: 0.01265\n",
      "Epoch: 148, Time: 0.00754s, Loss: 0.01365\n",
      "Epoch: 149, Time: 0.00731s, Loss: 0.01396\n",
      "Epoch: 150, Time: 0.00737s, Loss: 0.01298\n",
      "Epoch: 151, Time: 0.00775s, Loss: 0.01294\n",
      "Epoch: 152, Time: 0.00742s, Loss: 0.01285\n",
      "Epoch: 153, Time: 0.00756s, Loss: 0.01363\n",
      "Epoch: 154, Time: 0.00738s, Loss: 0.01327\n",
      "Epoch: 155, Time: 0.00739s, Loss: 0.01255\n",
      "Epoch: 156, Time: 0.00754s, Loss: 0.01272\n",
      "Epoch: 157, Time: 0.00736s, Loss: 0.01262\n",
      "Epoch: 158, Time: 0.00756s, Loss: 0.01346\n",
      "Epoch: 159, Time: 0.00742s, Loss: 0.01244\n",
      "Epoch: 160, Time: 0.00735s, Loss: 0.01412\n",
      "Epoch: 161, Time: 0.00804s, Loss: 0.01263\n",
      "Epoch: 162, Time: 0.00731s, Loss: 0.01296\n",
      "Epoch: 163, Time: 0.00741s, Loss: 0.01197\n",
      "Epoch: 164, Time: 0.00751s, Loss: 0.01273\n",
      "Epoch: 165, Time: 0.00737s, Loss: 0.01234\n",
      "Epoch: 166, Time: 0.00754s, Loss: 0.01260\n",
      "Epoch: 167, Time: 0.00734s, Loss: 0.01147\n",
      "Epoch: 168, Time: 0.00733s, Loss: 0.01318\n",
      "Epoch: 169, Time: 0.00752s, Loss: 0.01257\n",
      "Epoch: 170, Time: 0.00738s, Loss: 0.01249\n",
      "Epoch: 171, Time: 0.00750s, Loss: 0.01361\n",
      "Epoch: 172, Time: 0.00734s, Loss: 0.01169\n",
      "Epoch: 173, Time: 0.00756s, Loss: 0.01262\n",
      "Epoch: 174, Time: 0.00769s, Loss: 0.01278\n",
      "Epoch: 175, Time: 0.00743s, Loss: 0.01141\n",
      "Epoch: 176, Time: 0.00758s, Loss: 0.01323\n",
      "Epoch: 177, Time: 0.00732s, Loss: 0.01195\n",
      "Epoch: 178, Time: 0.00735s, Loss: 0.01289\n",
      "Epoch: 179, Time: 0.00755s, Loss: 0.01169\n",
      "Epoch: 180, Time: 0.00735s, Loss: 0.01220\n",
      "Epoch: 181, Time: 0.00736s, Loss: 0.01258\n",
      "Epoch: 182, Time: 0.00782s, Loss: 0.01171\n",
      "Epoch: 183, Time: 0.00774s, Loss: 0.01255\n",
      "Epoch: 184, Time: 0.00769s, Loss: 0.01318\n",
      "Epoch: 185, Time: 0.00736s, Loss: 0.01180\n",
      "Epoch: 186, Time: 0.00737s, Loss: 0.01220\n",
      "Epoch: 187, Time: 0.00755s, Loss: 0.01116\n",
      "Epoch: 188, Time: 0.00736s, Loss: 0.01374\n",
      "Epoch: 189, Time: 0.00745s, Loss: 0.01206\n",
      "Epoch: 190, Time: 0.00731s, Loss: 0.01393\n",
      "Epoch: 191, Time: 0.00735s, Loss: 0.01225\n",
      "Epoch: 192, Time: 0.00753s, Loss: 0.02270\n",
      "Epoch: 193, Time: 0.00734s, Loss: 0.09022\n",
      "Epoch: 194, Time: 0.00764s, Loss: 0.01545\n",
      "Epoch: 195, Time: 0.00737s, Loss: 0.07528\n",
      "Epoch: 196, Time: 0.00739s, Loss: 0.01831\n",
      "Epoch: 197, Time: 0.00756s, Loss: 0.03534\n",
      "Epoch: 198, Time: 0.00743s, Loss: 0.03308\n",
      "Epoch: 199, Time: 0.00864s, Loss: 0.02245\n",
      "\n",
      "train finished!\n",
      "best val: 0.52500\n",
      "test...\n",
      "final result: epoch: 140\n",
      "{'accuracy': 0.5139226317405701, 'f1_score': 0.3904649347897282, 'f1_score -> average@micro': 0.5139226045980294}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    set_seed(2021)\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    ################################################################\n",
    "    data = Cooking200()\n",
    "    G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "    X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "\n",
    "    train_mask = data[\"train_mask\"]\n",
    "    val_mask   = data[\"val_mask\"]\n",
    "    test_mask  = data[\"test_mask\"]\n",
    "\n",
    "    net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "    ################################################################\n",
    "    G = G.to(device)\n",
    "    X, lbl = X.to(device), lbl.to(device)\n",
    "    net = net.to(device)\n",
    "\n",
    "    ################################################################\n",
    "    best_val = 0\n",
    "    best_state = None\n",
    "    best_epoch = 0\n",
    "    for epoch in range(200):\n",
    "\n",
    "        # train, 每轮optimizer的参数在优化\n",
    "        train(G, X, lbl, net, train_mask, optimizer, epoch)\n",
    "\n",
    "        # validation\n",
    "        if epoch % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                val_res = infer(G, X, lbl, net, val_mask)\n",
    "            if val_res > best_val:\n",
    "                print(f\"update best: {val_res:.5f}\")\n",
    "                best_val   = val_res\n",
    "                best_state = deepcopy(net.state_dict())\n",
    "                best_epoch = epoch\n",
    "\n",
    "    print(\"\\ntrain finished!\")\n",
    "    print(f\"best val: {best_val:.5f}\")\n",
    "\n",
    "    # test\n",
    "    print(\"test...\")\n",
    "    net.load_state_dict(best_state)\n",
    "    res = infer(G, X, lbl, net, test_mask, test=True)\n",
    "    print(f\"final result: epoch: {best_epoch}\")\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HGNN_DHG",
   "language": "python",
   "name": "hgnn_dhg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
