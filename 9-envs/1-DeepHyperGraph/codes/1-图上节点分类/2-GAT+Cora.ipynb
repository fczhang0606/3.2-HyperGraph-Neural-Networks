{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051272da-a063-40af-b31f-11f35184cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from dhg.data import Cora\n",
    "from dhg import Graph\n",
    "from dhg.models import GAT\n",
    "from dhg.metrics import GraphVertexClassificationEvaluator as Evaluator\n",
    "from dhg.random import set_seed\n",
    "\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c00defd-ceba-49b0-aaf8-fb9999352c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(A, X, lbls, net, train_idx, optimizer, epoch):\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    st = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Time: {time.time()-st:.5f}s, Loss: {loss.item():.5f}\")\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e66f07-eacc-4ddb-aa3b-ac02e3a06e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer(A, X, lbls, net, idx, test=False):\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc3d9e7f-d5ed-4697-b801-08c0302a69c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.32923s, Loss: 1.94815\n",
      "update best: 0.12200\n",
      "Epoch: 1, Time: 0.01027s, Loss: 1.94413\n",
      "Epoch: 2, Time: 0.01060s, Loss: 1.93961\n",
      "Epoch: 3, Time: 0.01084s, Loss: 1.93500\n",
      "Epoch: 4, Time: 0.01072s, Loss: 1.93154\n",
      "update best: 0.14000\n",
      "Epoch: 5, Time: 0.01077s, Loss: 1.92676\n",
      "update best: 0.18000\n",
      "Epoch: 6, Time: 0.01021s, Loss: 1.91839\n",
      "update best: 0.30000\n",
      "Epoch: 7, Time: 0.01011s, Loss: 1.91322\n",
      "update best: 0.48800\n",
      "Epoch: 8, Time: 0.01005s, Loss: 1.91055\n",
      "update best: 0.56800\n",
      "Epoch: 9, Time: 0.00971s, Loss: 1.89851\n",
      "update best: 0.59200\n",
      "Epoch: 10, Time: 0.01029s, Loss: 1.89522\n",
      "update best: 0.65000\n",
      "Epoch: 11, Time: 0.01035s, Loss: 1.88976\n",
      "update best: 0.66000\n",
      "Epoch: 12, Time: 0.01025s, Loss: 1.87861\n",
      "update best: 0.68400\n",
      "Epoch: 13, Time: 0.00979s, Loss: 1.87413\n",
      "Epoch: 14, Time: 0.00986s, Loss: 1.86477\n",
      "update best: 0.69600\n",
      "Epoch: 15, Time: 0.01024s, Loss: 1.85221\n",
      "update best: 0.72200\n",
      "Epoch: 16, Time: 0.01057s, Loss: 1.85074\n",
      "update best: 0.73800\n",
      "Epoch: 17, Time: 0.01051s, Loss: 1.84227\n",
      "update best: 0.75400\n",
      "Epoch: 18, Time: 0.01037s, Loss: 1.82283\n",
      "update best: 0.77000\n",
      "Epoch: 19, Time: 0.01044s, Loss: 1.81943\n",
      "update best: 0.77400\n",
      "Epoch: 20, Time: 0.01020s, Loss: 1.80844\n",
      "update best: 0.78000\n",
      "Epoch: 21, Time: 0.00958s, Loss: 1.79395\n",
      "update best: 0.79200\n",
      "Epoch: 22, Time: 0.00959s, Loss: 1.77997\n",
      "Epoch: 23, Time: 0.00995s, Loss: 1.75401\n",
      "Epoch: 24, Time: 0.01019s, Loss: 1.75776\n",
      "Epoch: 25, Time: 0.01013s, Loss: 1.73037\n",
      "Epoch: 26, Time: 0.00934s, Loss: 1.73425\n",
      "update best: 0.79600\n",
      "Epoch: 27, Time: 0.00946s, Loss: 1.70365\n",
      "Epoch: 28, Time: 0.00996s, Loss: 1.69437\n",
      "Epoch: 29, Time: 0.01127s, Loss: 1.68958\n",
      "Epoch: 30, Time: 0.01093s, Loss: 1.65949\n",
      "Epoch: 31, Time: 0.01043s, Loss: 1.64409\n",
      "Epoch: 32, Time: 0.00947s, Loss: 1.61989\n",
      "Epoch: 33, Time: 0.00939s, Loss: 1.60972\n",
      "Epoch: 34, Time: 0.01018s, Loss: 1.58421\n",
      "Epoch: 35, Time: 0.01009s, Loss: 1.57437\n",
      "Epoch: 36, Time: 0.01016s, Loss: 1.55142\n",
      "Epoch: 37, Time: 0.00948s, Loss: 1.52626\n",
      "Epoch: 38, Time: 0.00946s, Loss: 1.48679\n",
      "Epoch: 39, Time: 0.00998s, Loss: 1.49335\n",
      "Epoch: 40, Time: 0.01017s, Loss: 1.47756\n",
      "Epoch: 41, Time: 0.01013s, Loss: 1.44734\n",
      "Epoch: 42, Time: 0.00954s, Loss: 1.42187\n",
      "Epoch: 43, Time: 0.00940s, Loss: 1.40552\n",
      "Epoch: 44, Time: 0.01040s, Loss: 1.36299\n",
      "Epoch: 45, Time: 0.01092s, Loss: 1.36297\n",
      "Epoch: 46, Time: 0.01015s, Loss: 1.32878\n",
      "Epoch: 47, Time: 0.00950s, Loss: 1.31232\n",
      "Epoch: 48, Time: 0.00937s, Loss: 1.28129\n",
      "Epoch: 49, Time: 0.00950s, Loss: 1.24890\n",
      "Epoch: 50, Time: 0.01011s, Loss: 1.24468\n",
      "Epoch: 51, Time: 0.01029s, Loss: 1.22721\n",
      "Epoch: 52, Time: 0.00949s, Loss: 1.21481\n",
      "Epoch: 53, Time: 0.00944s, Loss: 1.16354\n",
      "Epoch: 54, Time: 0.00943s, Loss: 1.17266\n",
      "Epoch: 55, Time: 0.01011s, Loss: 1.07577\n",
      "Epoch: 56, Time: 0.01020s, Loss: 1.15495\n",
      "Epoch: 57, Time: 0.00951s, Loss: 1.05832\n",
      "Epoch: 58, Time: 0.00937s, Loss: 1.01465\n",
      "Epoch: 59, Time: 0.01027s, Loss: 1.03845\n",
      "Epoch: 60, Time: 0.01112s, Loss: 1.00426\n",
      "Epoch: 61, Time: 0.01030s, Loss: 0.98591\n",
      "Epoch: 62, Time: 0.01010s, Loss: 1.01756\n",
      "Epoch: 63, Time: 0.00949s, Loss: 0.95988\n",
      "Epoch: 64, Time: 0.00959s, Loss: 0.92990\n",
      "Epoch: 65, Time: 0.01014s, Loss: 0.88626\n",
      "Epoch: 66, Time: 0.01010s, Loss: 0.88933\n",
      "Epoch: 67, Time: 0.01013s, Loss: 0.86820\n",
      "Epoch: 68, Time: 0.00949s, Loss: 0.88711\n",
      "Epoch: 69, Time: 0.00952s, Loss: 0.83543\n",
      "Epoch: 70, Time: 0.01035s, Loss: 0.80172\n",
      "Epoch: 71, Time: 0.01022s, Loss: 0.80175\n",
      "Epoch: 72, Time: 0.01012s, Loss: 0.78237\n",
      "Epoch: 73, Time: 0.00943s, Loss: 0.77646\n",
      "Epoch: 74, Time: 0.00991s, Loss: 0.71909\n",
      "Epoch: 75, Time: 0.00996s, Loss: 0.74149\n",
      "Epoch: 76, Time: 0.01011s, Loss: 0.73048\n",
      "Epoch: 77, Time: 0.01010s, Loss: 0.70157\n",
      "Epoch: 78, Time: 0.00948s, Loss: 0.66883\n",
      "Epoch: 79, Time: 0.00947s, Loss: 0.72277\n",
      "Epoch: 80, Time: 0.00935s, Loss: 0.65849\n",
      "Epoch: 81, Time: 0.01014s, Loss: 0.65905\n",
      "Epoch: 82, Time: 0.01010s, Loss: 0.59660\n",
      "Epoch: 83, Time: 0.00943s, Loss: 0.68735\n",
      "Epoch: 84, Time: 0.00950s, Loss: 0.58542\n",
      "Epoch: 85, Time: 0.00987s, Loss: 0.60081\n",
      "Epoch: 86, Time: 0.01010s, Loss: 0.60371\n",
      "Epoch: 87, Time: 0.01017s, Loss: 0.60648\n",
      "Epoch: 88, Time: 0.00941s, Loss: 0.58232\n",
      "Epoch: 89, Time: 0.00951s, Loss: 0.60412\n",
      "Epoch: 90, Time: 0.01055s, Loss: 0.58666\n",
      "Epoch: 91, Time: 0.01082s, Loss: 0.59403\n",
      "Epoch: 92, Time: 0.01022s, Loss: 0.52989\n",
      "update best: 0.79800\n",
      "Epoch: 93, Time: 0.01020s, Loss: 0.50727\n",
      "Epoch: 94, Time: 0.00947s, Loss: 0.54829\n",
      "Epoch: 95, Time: 0.00943s, Loss: 0.60227\n",
      "Epoch: 96, Time: 0.01012s, Loss: 0.54310\n",
      "Epoch: 97, Time: 0.01018s, Loss: 0.51440\n",
      "Epoch: 98, Time: 0.01004s, Loss: 0.54814\n",
      "Epoch: 99, Time: 0.00946s, Loss: 0.52012\n",
      "Epoch: 100, Time: 0.00947s, Loss: 0.50267\n",
      "Epoch: 101, Time: 0.01010s, Loss: 0.49638\n",
      "Epoch: 102, Time: 0.01022s, Loss: 0.50377\n",
      "Epoch: 103, Time: 0.01023s, Loss: 0.48407\n",
      "Epoch: 104, Time: 0.00942s, Loss: 0.48211\n",
      "Epoch: 105, Time: 0.01000s, Loss: 0.49930\n",
      "Epoch: 106, Time: 0.01080s, Loss: 0.50325\n",
      "Epoch: 107, Time: 0.01083s, Loss: 0.50455\n",
      "Epoch: 108, Time: 0.01027s, Loss: 0.46976\n",
      "Epoch: 109, Time: 0.01016s, Loss: 0.48681\n",
      "Epoch: 110, Time: 0.00944s, Loss: 0.52810\n",
      "Epoch: 111, Time: 0.00963s, Loss: 0.43823\n",
      "Epoch: 112, Time: 0.00955s, Loss: 0.45029\n",
      "Epoch: 113, Time: 0.01016s, Loss: 0.44791\n",
      "Epoch: 114, Time: 0.01030s, Loss: 0.47758\n",
      "Epoch: 115, Time: 0.00950s, Loss: 0.44956\n",
      "Epoch: 116, Time: 0.00952s, Loss: 0.45119\n",
      "Epoch: 117, Time: 0.00950s, Loss: 0.43624\n",
      "Epoch: 118, Time: 0.01033s, Loss: 0.45198\n",
      "Epoch: 119, Time: 0.01020s, Loss: 0.38180\n",
      "Epoch: 120, Time: 0.00997s, Loss: 0.42436\n",
      "Epoch: 121, Time: 0.01076s, Loss: 0.43104\n",
      "Epoch: 122, Time: 0.01002s, Loss: 0.41242\n",
      "Epoch: 123, Time: 0.00952s, Loss: 0.43909\n",
      "Epoch: 124, Time: 0.01019s, Loss: 0.39800\n",
      "Epoch: 125, Time: 0.01056s, Loss: 0.39631\n",
      "Epoch: 126, Time: 0.01014s, Loss: 0.39692\n",
      "Epoch: 127, Time: 0.00949s, Loss: 0.38694\n",
      "Epoch: 128, Time: 0.00937s, Loss: 0.44214\n",
      "Epoch: 129, Time: 0.01009s, Loss: 0.39690\n",
      "Epoch: 130, Time: 0.01012s, Loss: 0.38603\n",
      "Epoch: 131, Time: 0.01010s, Loss: 0.35502\n",
      "Epoch: 132, Time: 0.00958s, Loss: 0.34884\n",
      "Epoch: 133, Time: 0.00975s, Loss: 0.42068\n",
      "Epoch: 134, Time: 0.01022s, Loss: 0.38546\n",
      "Epoch: 135, Time: 0.01076s, Loss: 0.40577\n",
      "Epoch: 136, Time: 0.01057s, Loss: 0.37864\n",
      "Epoch: 137, Time: 0.01048s, Loss: 0.34676\n",
      "Epoch: 138, Time: 0.00972s, Loss: 0.40528\n",
      "Epoch: 139, Time: 0.00958s, Loss: 0.37687\n",
      "Epoch: 140, Time: 0.01012s, Loss: 0.35569\n",
      "Epoch: 141, Time: 0.01020s, Loss: 0.37773\n",
      "Epoch: 142, Time: 0.01016s, Loss: 0.34510\n",
      "Epoch: 143, Time: 0.00952s, Loss: 0.38751\n",
      "Epoch: 144, Time: 0.00963s, Loss: 0.37304\n",
      "Epoch: 145, Time: 0.01015s, Loss: 0.36880\n",
      "Epoch: 146, Time: 0.01028s, Loss: 0.39444\n",
      "Epoch: 147, Time: 0.01017s, Loss: 0.37824\n",
      "Epoch: 148, Time: 0.00957s, Loss: 0.35609\n",
      "Epoch: 149, Time: 0.00991s, Loss: 0.40778\n",
      "Epoch: 150, Time: 0.01051s, Loss: 0.36518\n",
      "Epoch: 151, Time: 0.01069s, Loss: 0.33235\n",
      "Epoch: 152, Time: 0.01027s, Loss: 0.33207\n",
      "Epoch: 153, Time: 0.01178s, Loss: 0.38934\n",
      "Epoch: 154, Time: 0.01146s, Loss: 0.34042\n",
      "Epoch: 155, Time: 0.00986s, Loss: 0.36693\n",
      "Epoch: 156, Time: 0.00955s, Loss: 0.34734\n",
      "Epoch: 157, Time: 0.00956s, Loss: 0.35648\n",
      "Epoch: 158, Time: 0.01011s, Loss: 0.33347\n",
      "Epoch: 159, Time: 0.01013s, Loss: 0.32242\n",
      "Epoch: 160, Time: 0.00941s, Loss: 0.33993\n",
      "Epoch: 161, Time: 0.00952s, Loss: 0.32605\n",
      "Epoch: 162, Time: 0.00952s, Loss: 0.30478\n",
      "Epoch: 163, Time: 0.01013s, Loss: 0.34807\n",
      "Epoch: 164, Time: 0.01028s, Loss: 0.34922\n",
      "Epoch: 165, Time: 0.00985s, Loss: 0.35867\n",
      "Epoch: 166, Time: 0.00970s, Loss: 0.34352\n",
      "Epoch: 167, Time: 0.00999s, Loss: 0.35200\n",
      "Epoch: 168, Time: 0.01189s, Loss: 0.37003\n",
      "Epoch: 169, Time: 0.01074s, Loss: 0.37276\n",
      "Epoch: 170, Time: 0.01017s, Loss: 0.33657\n",
      "Epoch: 171, Time: 0.01025s, Loss: 0.31496\n",
      "Epoch: 172, Time: 0.00961s, Loss: 0.30342\n",
      "Epoch: 173, Time: 0.00951s, Loss: 0.34901\n",
      "Epoch: 174, Time: 0.01025s, Loss: 0.36342\n",
      "Epoch: 175, Time: 0.01006s, Loss: 0.30637\n",
      "Epoch: 176, Time: 0.01020s, Loss: 0.33495\n",
      "Epoch: 177, Time: 0.00955s, Loss: 0.28906\n",
      "Epoch: 178, Time: 0.00965s, Loss: 0.28243\n",
      "Epoch: 179, Time: 0.00958s, Loss: 0.30446\n",
      "Epoch: 180, Time: 0.01104s, Loss: 0.29046\n",
      "Epoch: 181, Time: 0.01053s, Loss: 0.31503\n",
      "Epoch: 182, Time: 0.01051s, Loss: 0.35520\n",
      "Epoch: 183, Time: 0.00959s, Loss: 0.30877\n",
      "Epoch: 184, Time: 0.00967s, Loss: 0.30470\n",
      "Epoch: 185, Time: 0.00966s, Loss: 0.31141\n",
      "Epoch: 186, Time: 0.01019s, Loss: 0.28867\n",
      "Epoch: 187, Time: 0.01020s, Loss: 0.30242\n",
      "Epoch: 188, Time: 0.00959s, Loss: 0.29352\n",
      "Epoch: 189, Time: 0.00965s, Loss: 0.27466\n",
      "Epoch: 190, Time: 0.00957s, Loss: 0.31637\n",
      "Epoch: 191, Time: 0.01015s, Loss: 0.27062\n",
      "Epoch: 192, Time: 0.01019s, Loss: 0.29271\n",
      "Epoch: 193, Time: 0.00958s, Loss: 0.28072\n",
      "Epoch: 194, Time: 0.00958s, Loss: 0.29671\n",
      "Epoch: 195, Time: 0.01056s, Loss: 0.27922\n",
      "Epoch: 196, Time: 0.01138s, Loss: 0.28615\n",
      "Epoch: 197, Time: 0.01122s, Loss: 0.25760\n",
      "Epoch: 198, Time: 0.01052s, Loss: 0.27872\n",
      "Epoch: 199, Time: 0.01025s, Loss: 0.29232\n",
      "Epoch: 200, Time: 0.00964s, Loss: 0.31783\n",
      "Epoch: 201, Time: 0.00960s, Loss: 0.30943\n",
      "Epoch: 202, Time: 0.00953s, Loss: 0.30592\n",
      "Epoch: 203, Time: 0.01029s, Loss: 0.29353\n",
      "Epoch: 204, Time: 0.01021s, Loss: 0.27466\n",
      "Epoch: 205, Time: 0.00956s, Loss: 0.28536\n",
      "Epoch: 206, Time: 0.00963s, Loss: 0.27443\n",
      "Epoch: 207, Time: 0.00956s, Loss: 0.29530\n",
      "Epoch: 208, Time: 0.01020s, Loss: 0.34877\n",
      "Epoch: 209, Time: 0.01017s, Loss: 0.33556\n",
      "Epoch: 210, Time: 0.01092s, Loss: 0.30290\n",
      "Epoch: 211, Time: 0.00968s, Loss: 0.30490\n",
      "Epoch: 212, Time: 0.01006s, Loss: 0.24114\n",
      "Epoch: 213, Time: 0.00970s, Loss: 0.28710\n",
      "Epoch: 214, Time: 0.01015s, Loss: 0.28103\n",
      "Epoch: 215, Time: 0.01030s, Loss: 0.30953\n",
      "Epoch: 216, Time: 0.00958s, Loss: 0.26583\n",
      "Epoch: 217, Time: 0.00974s, Loss: 0.31849\n",
      "Epoch: 218, Time: 0.00974s, Loss: 0.27394\n",
      "Epoch: 219, Time: 0.01034s, Loss: 0.31158\n",
      "Epoch: 220, Time: 0.01023s, Loss: 0.24306\n",
      "Epoch: 221, Time: 0.01023s, Loss: 0.27975\n",
      "Epoch: 222, Time: 0.00951s, Loss: 0.29307\n",
      "Epoch: 223, Time: 0.00957s, Loss: 0.24466\n",
      "Epoch: 224, Time: 0.01021s, Loss: 0.25802\n",
      "Epoch: 225, Time: 0.01083s, Loss: 0.26903\n",
      "Epoch: 226, Time: 0.01094s, Loss: 0.29384\n",
      "Epoch: 227, Time: 0.01034s, Loss: 0.27130\n",
      "Epoch: 228, Time: 0.00957s, Loss: 0.28914\n",
      "Epoch: 229, Time: 0.00960s, Loss: 0.28182\n",
      "Epoch: 230, Time: 0.01019s, Loss: 0.32852\n",
      "Epoch: 231, Time: 0.01016s, Loss: 0.25669\n",
      "Epoch: 232, Time: 0.01030s, Loss: 0.29292\n",
      "Epoch: 233, Time: 0.00972s, Loss: 0.24420\n",
      "Epoch: 234, Time: 0.00962s, Loss: 0.24628\n",
      "Epoch: 235, Time: 0.01014s, Loss: 0.24283\n",
      "Epoch: 236, Time: 0.01019s, Loss: 0.25664\n",
      "Epoch: 237, Time: 0.01029s, Loss: 0.28009\n",
      "Epoch: 238, Time: 0.00958s, Loss: 0.27697\n",
      "Epoch: 239, Time: 0.00963s, Loss: 0.27215\n",
      "Epoch: 240, Time: 0.01007s, Loss: 0.25050\n",
      "Epoch: 241, Time: 0.01096s, Loss: 0.28319\n",
      "Epoch: 242, Time: 0.01047s, Loss: 0.27778\n",
      "Epoch: 243, Time: 0.01028s, Loss: 0.23993\n",
      "Epoch: 244, Time: 0.00967s, Loss: 0.24299\n",
      "Epoch: 245, Time: 0.00961s, Loss: 0.25205\n",
      "Epoch: 246, Time: 0.01000s, Loss: 0.23747\n",
      "Epoch: 247, Time: 0.01041s, Loss: 0.29518\n",
      "Epoch: 248, Time: 0.01022s, Loss: 0.22905\n",
      "Epoch: 249, Time: 0.00958s, Loss: 0.27316\n",
      "Epoch: 250, Time: 0.00970s, Loss: 0.24729\n",
      "Epoch: 251, Time: 0.00962s, Loss: 0.27449\n",
      "Epoch: 252, Time: 0.01020s, Loss: 0.25081\n",
      "Epoch: 253, Time: 0.01016s, Loss: 0.24817\n",
      "Epoch: 254, Time: 0.00957s, Loss: 0.26140\n",
      "Epoch: 255, Time: 0.01033s, Loss: 0.25686\n",
      "Epoch: 256, Time: 0.01001s, Loss: 0.25517\n",
      "Epoch: 257, Time: 0.01023s, Loss: 0.23080\n",
      "Epoch: 258, Time: 0.01020s, Loss: 0.24169\n",
      "Epoch: 259, Time: 0.01033s, Loss: 0.22051\n",
      "Epoch: 260, Time: 0.00960s, Loss: 0.25122\n",
      "Epoch: 261, Time: 0.00957s, Loss: 0.24581\n",
      "Epoch: 262, Time: 0.00973s, Loss: 0.23206\n",
      "Epoch: 263, Time: 0.01021s, Loss: 0.24589\n",
      "Epoch: 264, Time: 0.01027s, Loss: 0.22829\n",
      "Epoch: 265, Time: 0.01032s, Loss: 0.20756\n",
      "Epoch: 266, Time: 0.00963s, Loss: 0.29324\n",
      "Epoch: 267, Time: 0.00960s, Loss: 0.22570\n",
      "Epoch: 268, Time: 0.01019s, Loss: 0.24070\n",
      "Epoch: 269, Time: 0.01051s, Loss: 0.24533\n",
      "Epoch: 270, Time: 0.01052s, Loss: 0.29275\n",
      "Epoch: 271, Time: 0.00974s, Loss: 0.24174\n",
      "Epoch: 272, Time: 0.00968s, Loss: 0.26233\n",
      "Epoch: 273, Time: 0.00963s, Loss: 0.22656\n",
      "Epoch: 274, Time: 0.01015s, Loss: 0.21420\n",
      "Epoch: 275, Time: 0.01032s, Loss: 0.23820\n",
      "Epoch: 276, Time: 0.01025s, Loss: 0.23800\n",
      "Epoch: 277, Time: 0.00961s, Loss: 0.21346\n",
      "Epoch: 278, Time: 0.00956s, Loss: 0.22294\n",
      "Epoch: 279, Time: 0.01024s, Loss: 0.26511\n",
      "Epoch: 280, Time: 0.01027s, Loss: 0.26094\n",
      "Epoch: 281, Time: 0.01025s, Loss: 0.27199\n",
      "Epoch: 282, Time: 0.00964s, Loss: 0.26433\n",
      "Epoch: 283, Time: 0.00951s, Loss: 0.20396\n",
      "Epoch: 284, Time: 0.01028s, Loss: 0.23454\n",
      "Epoch: 285, Time: 0.01046s, Loss: 0.25586\n",
      "Epoch: 286, Time: 0.01170s, Loss: 0.26680\n",
      "Epoch: 287, Time: 0.01142s, Loss: 0.23314\n",
      "Epoch: 288, Time: 0.01103s, Loss: 0.24275\n",
      "Epoch: 289, Time: 0.01148s, Loss: 0.27589\n",
      "Epoch: 290, Time: 0.00965s, Loss: 0.24477\n",
      "Epoch: 291, Time: 0.00979s, Loss: 0.25350\n",
      "Epoch: 292, Time: 0.00964s, Loss: 0.21704\n",
      "Epoch: 293, Time: 0.01041s, Loss: 0.19887\n",
      "Epoch: 294, Time: 0.01028s, Loss: 0.22608\n",
      "Epoch: 295, Time: 0.01025s, Loss: 0.28010\n",
      "Epoch: 296, Time: 0.00961s, Loss: 0.22907\n",
      "Epoch: 297, Time: 0.00955s, Loss: 0.24766\n",
      "Epoch: 298, Time: 0.01033s, Loss: 0.26830\n",
      "Epoch: 299, Time: 0.01027s, Loss: 0.27138\n",
      "\n",
      "train finished!\n",
      "best val: 0.79800\n",
      "test...\n",
      "final result: epoch: 92\n",
      "{'accuracy': 0.8149999976158142, 'f1_score': 0.8073151298375543, 'f1_score -> average@micro': 0.815}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    set_seed(2022)\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    ################################################################\n",
    "    data = Cora()\n",
    "    G = Graph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "    X, lbl = data[\"features\"], data[\"labels\"]\n",
    "\n",
    "    train_mask = data[\"train_mask\"]\n",
    "    val_mask   = data[\"val_mask\"]\n",
    "    test_mask  = data[\"test_mask\"]\n",
    "\n",
    "    net = GAT(data[\"dim_features\"], 8, data[\"num_classes\"], num_heads=8, drop_rate=0.6)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "    evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "    ################################################################\n",
    "    G = G.to(device)\n",
    "    # X, lbl = X.to(device), lbl.to(device)\n",
    "    X, lbl = X.cuda(), lbl.cuda()\n",
    "    # net = net.to(device)\n",
    "    net = net.cuda()\n",
    "\n",
    "    ################################################################\n",
    "    best_val = 0\n",
    "    best_state = None\n",
    "    best_epoch = 0\n",
    "    for epoch in range(300):\n",
    "\n",
    "        # train, 每轮optimizer的参数在优化\n",
    "        train(G, X, lbl, net, train_mask, optimizer, epoch)\n",
    "\n",
    "        # validation\n",
    "        if epoch % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                val_res = infer(G, X, lbl, net, val_mask)\n",
    "            if val_res > best_val:\n",
    "                print(f\"update best: {val_res:.5f}\")\n",
    "                best_val   = val_res\n",
    "                best_state = deepcopy(net.state_dict())\n",
    "                best_epoch = epoch\n",
    "\n",
    "    print(\"\\ntrain finished!\")\n",
    "    print(f\"best val: {best_val:.5f}\")\n",
    "\n",
    "    # test\n",
    "    print(\"test...\")\n",
    "    net.load_state_dict(best_state)\n",
    "    res = infer(G, X, lbl, net, test_mask, test=True)\n",
    "    print(f\"final result: epoch: {best_epoch}\")\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HGNN_DHG",
   "language": "python",
   "name": "hgnn_dhg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
